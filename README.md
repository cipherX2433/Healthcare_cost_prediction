# ğŸ¥ Health Insurance Premium Prediction - Advanced ML Pipeline

This project focuses on **predicting annual health insurance premiums** using demographic, financial, and health-related features. It includes robust **EDA**, **Feature Engineering**, **Model Training**, and **Error Analysis**, across **three customer segments**:

<p align="center">
  ğŸŒ <a href="https://cipherx-healthcare-cost-prediction.streamlit.app/">Live Demo App</a> |
  ğŸ§  <a href="https://github.com/cipherX2433/Healthcare_cost_prediction">Source Code</a>
</p>

![Demo](ss1.png)

- `premiums.xlsx` â†’ All users  
- `premiums_rest.xlsx` â†’ Excluding young users  
- `premiums_young.xlsx` â†’ Young segment (age â‰¤ 25)

---

## ğŸ“ Dataset Overview

| Feature                | Description                                 |
|------------------------|---------------------------------------------|
| `age`                  | Age of policyholder                         |
| `gender`               | Gender                                      |
| `region`               | Geographic region                           |
| `income_lakhs`         | Income in Lakhs                             |
| `income_level`         | Binned income level                         |
| `employment_status`    | Employment type                             |
| `smoking_status`       | Smoker status                               |
| `medical_history`      | Health condition(s)                         |
| `insurance_plan`       | Bronze/Silver/Gold tier                     |
| `annual_premium_amount`| ğŸ’¡ **Target Variable**                      |

---

## ğŸ”„ Data Preprocessing

- âœ… Removed nulls and duplicates  
- ğŸ”„ Fixed negative `number_of_dependants`  
- âœ‚ï¸ Capped outliers using **IQR** and **quantile thresholds**  
- ğŸ”§ Cleaned dirty categorical values (e.g. `'Smoking=0'` â†’ `'No Smoking'`)

---

## ğŸ“Š Exploratory Data Analysis (EDA)

- ğŸ“¦ **Boxplots** â†’ Outlier detection  
- ğŸ“ˆ **Histograms (with KDE)** â†’ Skewness & distribution  
- ğŸŸ¢ **Scatterplots** â†’ Continuous vs Target  
- ğŸ“Š **Barplots & Heatmaps** â†’ Categorical impact  

---

## ğŸ§  Feature Engineering

- ğŸ§¬ Parsed `medical_history` into `disease1`, `disease2`
- ğŸ§® Assigned **Risk Scores** based on disease:
  ```python
  risk_score = {
    "diabetes": 6, "heart disease": 8, "high blood pressure": 6,
    "thyroid": 5, "no disease": 0, "none": 0
  }
## âš™ï¸ Feature Engineering

- â• Created `total_risk_score` by combining disease severity scores
- ğŸ“Š Normalized the risk score between 0 and 1
- ğŸ”¢ Encoded ordinal features:
  - `insurance_plan`: Bronze â†’ 1, Silver â†’ 2, Gold â†’ 3
  - `income_level`: <10L â†’ 1, ..., >40L â†’ 4
- ğŸ” One-hot encoded all nominal categorical features:
  - `gender`, `region`, `bmi_category`, `marital_status`, etc.

---

## ğŸ“ Correlation & Multicollinearity

- ğŸ” Generated **correlation heatmap** for feature relationships
- ğŸ§  Used **Variance Inflation Factor (VIF)** for multicollinearity detection
  - âŒ Dropped `income_level` (due to high VIF)

---

## âš–ï¸ Feature Scaling

Used **`MinMaxScaler`** on the following continuous/ordinal variables:

```text
â€¢ age  
â€¢ number_of_dependants  
â€¢ income_lakhs  
â€¢ income_level  
â€¢ insurance_plan  
ğŸ¤– Model Training & Evaluation
ğŸ”¹ Linear Regression

âœ… Interpretable and stable model
ğŸ“ˆ Achieved high train/test RÂ² scores
ğŸ“Š Visualized feature importance using coefficients
ğŸ”¸ Ridge Regression

ğŸ›  Tested multiple alpha values: 0.01, 0.1, 1, 5, 10
ğŸ¯ Assessed impact of regularization on model performance
ğŸŸ  XGBoost Regressor

âœ… Best performing model (lowest RMSE, highest RÂ²)
ğŸ”§ Tuned with RandomizedSearchCV using parameter grid:
python
param_grid = {
    'n_estimators': [20, 40, 50],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
}
ğŸ“Š Tree-based feature importance is effective but less interpretable
ğŸ§ª Evaluation Metrics

Metric	Description
RÂ² Score	Goodness-of-fit (explained variance)
MSE / RMSE	Magnitude of prediction error
diff_pct	% error calculated as (predicted - actual) / actual * 100
âŒ Error Analysis

Strategy:
Predicted y_pred on test set X_test
Computed:
residual = predicted - actual
diff_pct = residual * 100 / actual
Focused on extreme errors where |diff_pct| > 10%
Key Findings:
Extreme errors are present in approximately X% of test data
Most high-error points are from users aged â‰¤ 25 years
These users cause prediction instability even after feature scaling
Visual Insight:
ğŸ”µ Blue = Extreme Errors
ğŸ”´ Red = Overall Test Distribution
Example: Age distribution with extreme error highlighting
![Age Distribution Error Histogram](https://github.com/yourusername/projectname/assets âœ… Segmented Modeling Solution
Segment	Dataset Used
General Users	premiums_rest.xlsx
Younger Users (â‰¤25)	premiums_young.xlsx
Segmented models significantly reduced prediction error and improved model generalizability.
ğŸ§© Future Improvements

ğŸ” Apply log or Box-Cox transformation on annual_premium_amount
ğŸ” Use SHAP or LIME for better XGBoost interpretability
ğŸ§  Develop stacked or ensemble models for improved accuracy
ğŸ’¡ Add business logic constraints (e.g., cap predictions based on income tiers)
ğŸ›  Tech Stack

Tool / Library	Purpose
ğŸ Python	Core programming language
ğŸ§® Pandas, NumPy	Data processing
ğŸ“Š Seaborn, Matplotlib	Data visualization
ğŸ¤– Scikit-learn	ML models, metrics, preprocessing
ğŸŒ² XGBoost	Tree-based regression
ğŸ§  Statsmodels	VIF calculations & diagnostics
